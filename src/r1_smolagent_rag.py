import os
from dotenv import load_dotenv

# smolagents and langchain imports
from smolagents import OpenAIServerModel, CodeAgent, ToolCallingAgent, tool, GradioUI
from langchain_chroma import Chroma
from langchain.embeddings import AzureOpenAIEmbeddings  # Updated import

load_dotenv()

def get_model(model_id: str):
    """
    Returns an OpenAI model instance using OpenAIServerModel.
    Requires OPENAI_API_KEY to be set in environment variables.
    """
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("API_BASE", "https://api.sambanova.ai/v1")
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable must be set")
    
    return OpenAIServerModel(
        model_id=model_id,
        api_base=base_url,
        api_key=api_key
    )

# Fetch model IDs from environment - using OpenAI's latest models
reasoning_model_id = os.getenv("REASONING_MODEL_ID", "DeepSeek-R1-Distill-Llama-70B")  # Example reasoning model ID
tool_model_id = os.getenv("TOOL_MODEL_ID", "gpt-3.5-turbo")  # Example tool-calling model ID

# Create the reasoner model and agent
reasoning_model = get_model(reasoning_model_id)
reasoner = CodeAgent(
    tools=[],
    model=reasoning_model,
    add_base_tools=False,
    max_steps=2
)

# Initialize vector store and embeddings
script_dir = os.path.dirname(__file__)
db_dir = os.path.join(script_dir, "chroma_db")
embeddings = AzureOpenAIEmbeddings(
    azure_deployment=os.getenv("AZURE_EMBEDDINGS_MODEL"),
    openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    openai_api_base=os.getenv("AZURE_OPENAI_ENDPOINT"),
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    model=os.getenv("AZURE_EMBEDDINGS_MODEL")
)
vectordb = Chroma(
    persist_directory=db_dir,
    embedding_function=embeddings
)

@tool
def rag_with_reasoner(user_query: str) -> str:
    """
    Perform a Retrieval-Augmented Generation (RAG) query using the vector database 
    and a reasoning model.

    1. Search for the top 3 relevant chunks from the vector database.
    2. Create a prompt with the retrieved context.
    3. Use the reasoning model to generate a concise, specific answer.
       If insufficient context is found, the answer should suggest a refined query.

    Args:
        user_query: The user's question to query the vector database.

    Returns:
        A short answer (string) generated by the reasoning model based on retrieved context.
    """
    # Search for relevant documents
    docs = vectordb.similarity_search(user_query, k=3)
    
    # Combine document contents
    context = "\n\n".join(doc.page_content for doc in docs)
    
    # Create a prompt with the retrieved context to guide the reasoning model
    prompt = f"""You are a helpful assistant that uses the following context from our documents 
to answer the user’s question. If you do not find sufficient information in the context, 
you must respond with "I’m not sure, but here’s a better query to refine your search:" 
followed by a suggested refined query.

Context:
{context}

Question: {user_query}

Please provide a concise and specific answer based only on the context above. 
If the answer is not fully supported by the context, acknowledge that and consider 
suggesting a refined query.

Answer:
"""
    # Get response from the reasoning model
    response = reasoner.run(prompt, reset=False)
    return response

# Create the primary agent to direct the conversation
tool_model = get_model(tool_model_id)
primary_agent = ToolCallingAgent(
    tools=[rag_with_reasoner],
    model=tool_model,
    add_base_tools=False,
    max_steps=3
)

def main():
    """
    Launches a simple Gradio UI for testing the RAG system.
    """
    GradioUI(primary_agent).launch()

if __name__ == "__main__":
    main()
